{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "complete-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sweet-brand",
   "metadata": {},
   "source": [
    "Use the Shakespeare_data.csv. I would like to use bigger sample size.However, bigger sample size would exceed the size limit of numpy array. we only need to focus on the PlayerLine data since Player Line is what I want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "colored-audit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9274               That for my surety will refuse the boys!\n",
       "55700         Which shallow laughing hearers give to fools:\n",
       "110032                        Perfume for a lady's chamber,\n",
       "48233                                                Exeunt\n",
       "53936     'Gainst thee, thou lamb, that standest as his ...\n",
       "                                ...                        \n",
       "38302                 Almost no better than so many French,\n",
       "65640             Believe me, there is no such thing in me.\n",
       "35589                             An you call him a-down-a.\n",
       "51033     charity, whom the foul fiend vexes: there could I\n",
       "59434          Ha! fie, these filthy vices! It were as good\n",
       "Name: PlayerLine, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_data = pd.read_csv ('./Shakespeare_data.csv')\n",
    "shakespeare_data = shakespeare_data.sample(1000)\n",
    "shakespeare_data['PlayerLine']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "necessary-vacuum",
   "metadata": {},
   "source": [
    "Count number of words and unqiue words used in the dataset. Treate each unique word as a state in a matrix of state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bronze-studio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of words used in player lines: 7389\n",
      "Total of unique used among those words: 2871\n"
     ]
    }
   ],
   "source": [
    "new_words = []\n",
    "word_list = []\n",
    "playerLines = shakespeare_data['PlayerLine']\n",
    "for line in playerLines:\n",
    "    new_words.append(line.split()[0])\n",
    "    for word in line.split():\n",
    "        word_list.append(word)\n",
    "\n",
    "# for word in wordArray:\n",
    "#     print(word)\n",
    "unique_words = list(set(word_list))\n",
    "unique_words.sort()\n",
    "num_of_unique_words = len(unique_words)\n",
    "\n",
    "# Get total of words\n",
    "print(\"Total of words used in player lines:\", len(word_list))\n",
    "print(\"Total of unique used among those words:\", num_of_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "revolutionary-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dict.fromkeys(unique_words)\n",
    "index = 0\n",
    "for word in words:\n",
    "    words[word]=index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "corrected-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices of trasition probabilities of the two previous words\n",
    "First_Trans = np.zeros((num_of_unique_words + 1, num_of_unique_words + 1))\n",
    "Second_Trans = np.zeros((num_of_unique_words + 1, num_of_unique_words + 1))\n",
    "\n",
    "for playerLine in playerLines:\n",
    "    sentence = playerLine.split()\n",
    "    for i in range(len(sentence)):\n",
    "        if i < len(sentence) - 1:\n",
    "            First_Trans[words[sentence[i]]][words[sentence[i + 1]]] += 1\n",
    "        else:\n",
    "            First_Trans[words[sentence[i]]][num_of_unique_words] += 1\n",
    "\n",
    "        if i < len(sentence) - 2:\n",
    "            Second_Trans[words[sentence[i]]][words[sentence[i + 2]]] += 1\n",
    "        else:\n",
    "            Second_Trans[words[sentence[i]]][num_of_unique_words] += 1\n",
    "\n",
    "First_Trans[num_of_unique_words][num_of_unique_words] = 1\n",
    "Second_Trans[num_of_unique_words][num_of_unique_words] = 1\n",
    "\n",
    "First_Trans_Mat = First_Trans\n",
    "Second_Trans_Mat = Second_Trans\n",
    "\n",
    "for i in range(len(First_Trans)):\n",
    "    First_Trans_Mat[i] = First_Trans_Mat[i] / First_Trans_Mat[i].sum()\n",
    "    Second_Trans_Mat[i] = Second_Trans_Mat[i] / Second_Trans_Mat[i].sum()\n",
    "\n",
    "unique_words.append(None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "miniature-balloon",
   "metadata": {},
   "source": [
    "The idea is to implement Makrov Model to compute the next word, based on the probablilities transition of the last two states. Second-order Makrov Model. Hidden Makrov Model is assumed to be Makrov process with hidden (a.k.a unobserved) state. Therefore, I initally implemented Makrov Model, and I implemented Hidden Makrov Model later. I extended the Makrov Model with hidden states to become Hidden Makrov Model. My hidden states for this project are 'long' word and 'short' word. The average word length in English word length is 4.7 letters.Therefore, the word that is 4 or bellow letters is considered as 'Short'. Otherwise, that word is considered as 'Long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "miniature-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "# Emission probabilities\n",
    "def emission(prob, state):\n",
    "    for i in range(len(prob)-1):\n",
    "        if state == 'Short':\n",
    "            if len(unique_words[i]) < 5:\n",
    "                prob[i] = prob[i] * 2\n",
    "            else:\n",
    "                prob[i] = prob[i] / 2\n",
    "        else:\n",
    "            if len(unique_words[i]) > 4:\n",
    "                prob[i] = prob[i] * 2\n",
    "            else:\n",
    "                prob[i] = prob[i] / 2\n",
    "    prob[i] = prob[i] / prob.sum()\n",
    "    \n",
    "    \n",
    "Hiddent_State_Obj = {'Short': 0, 'Long': 1}\n",
    "Hiddent_States = ['Short','Long']\n",
    "Hidden_Mat = [[.8, .2],[.7, .3]]\n",
    "\n",
    "# Returns which of the Hidden State\n",
    "def nextState(Hidden_State):\n",
    "    nextState = np.random.choice(Hiddent_States, size = 1, p = Hidden_Mat[Hiddent_State_Obj[Hidden_State]])\n",
    "    return nextState[0]\n",
    "\n",
    "# Hidden Makrov Model implementation\n",
    "def HMM(word_choice = None, length = 7, sentence = [], hidden_State = 'Short'):\n",
    "    if word_choice is None:\n",
    "        word_choice = np.random.choice(new_words, size=1)[0]\n",
    "    if sentence == []:\n",
    "        sentence.append(word_choice)   \n",
    "    next_word = np.random.choice(unique_words, size = 1,p = First_Trans_Mat[words[sentence[-1]]])[0]\n",
    "    if length > 1:\n",
    "        while(next_word is None):\n",
    "            next_word = np.random.choice(unique_words, size = 1,p = First_Trans_Mat[words[sentence[-1]]])[0]\n",
    "\n",
    "    while next_word is not None:\n",
    "        sentence.append(next_word)\n",
    "        next_Probs = First_Trans_Mat[words[sentence[-1]]] * (Second_Trans_Mat[words[sentence[-2]]]) + First_Trans_Mat[words[sentence[-1]]]/4\n",
    "        next_prob = emission(next_Probs, hidden_State)\n",
    "        next_Probs[-1] += 0.00001\n",
    "        next_Probs = next_Probs / next_Probs.sum()\n",
    "        \n",
    "        if len(sentence) < length - 1:\n",
    "            if next_Probs.sum() > next_Probs[-1]:\n",
    "                next_Probs[-1] = next_Probs[-1] / 10\n",
    "            next_Probs = next_Probs / next_Probs.sum()\n",
    "            \n",
    "        if len(sentence) > length + 1:\n",
    "            next_Probs[-1] = next_Probs[-1] * 2\n",
    "            next_Probs = next_Probs / next_Probs.sum()\n",
    "        next_word = np.random.choice(unique_words, size = 1,p = next_Probs)[0]\n",
    "        \n",
    "        hidden_State = nextState(hidden_State)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hindu-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is true gentleness.\n"
     ]
    }
   ],
   "source": [
    "sentence = HMM()\n",
    "print(' '.join(str(x) for x in sentence if x is not None))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ignored-savage",
   "metadata": {},
   "source": [
    "Generate a sentence with given words \"That's the letter \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "public-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's the letter come from the skin of a pair of our general?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"That's the letter \"\n",
    "generatedSentence = HMM(word_choice = sentence.split()[-1], sentence = sentence.split())\n",
    "print(' '.join(str(x) for x in generatedSentence if x is not None))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "labeled-cookie",
   "metadata": {},
   "source": [
    "This generated sentence has a similar vibe of a player line written by Shakespeare. Thus, I'm pretty pleased with the current. However, I think the result could be better if I could try in a larger sample. 1000 lines is a relative small size sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-memorial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
