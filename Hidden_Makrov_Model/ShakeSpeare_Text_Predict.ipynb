{
 "cells": [
  {
   "cell_type": "raw",
   "id": "objective-departure",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cooperative-talent",
   "metadata": {},
   "source": [
    "# Use the Shakespeare_data.csv. I would like to use bigger sample size.However, bigger sample size would exceed the size limit of numpy array\n",
    "# we only need to focus on the PlayerLine data since Player Line is what I want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "tough-aggregate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46375             I have not from your eyes that gentleness\n",
       "94684               [Aside]  Lord Lucius and Lucullus? hum!\n",
       "45270        The enfranchisement of Arthur, whose restraint\n",
       "22719      And from the mart he's somewhere gone to dinner.\n",
       "74650        expostulate with her, lest her body and beauty\n",
       "                                ...                        \n",
       "46143        In their continuance will not feel themselves.\n",
       "103966    me she did affect me: and I have heard herself...\n",
       "98535                And will revolt from me to succor him.\n",
       "29946                   This tribute from us, we were free:\n",
       "78318             But curb it, spite of seeing. O, my lord,\n",
       "Name: PlayerLine, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_data = pd.read_csv ('./Shakespeare_data.csv')\n",
    "shakespeare_data = shakespeare_data.sample(1000)\n",
    "shakespeare_data['PlayerLine']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "necessary-vacuum",
   "metadata": {},
   "source": [
    "# Count number of words and unqiue words used in the dataset\n",
    "# Treate each unique word as a state in a matrix of state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "bronze-studio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of words used in player lines: 14556\n",
      "Total of unique used among those words: 2853\n"
     ]
    }
   ],
   "source": [
    "new_words = []\n",
    "word_list = []\n",
    "playerLines = shakespeare_data['PlayerLine']\n",
    "for line in playerLines:\n",
    "    new_words.append(line.split()[0])\n",
    "    for word in line.split():\n",
    "        word_list.append(word)\n",
    "\n",
    "# for word in wordArray:\n",
    "#     print(word)\n",
    "unique_words = list(set(word_list))\n",
    "unique_words.sort()\n",
    "num_of_unique_words = len(unique_words)\n",
    "\n",
    "# Get total of words\n",
    "print(\"Total of words used in player lines:\", len(wordArray))\n",
    "print(\"Total of unique used among those words:\", num_of_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "revolutionary-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dict.fromkeys(unique_words)\n",
    "index = 0\n",
    "for word in words:\n",
    "    words[word]=index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "corrected-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices of trasition probabilities of the two previous words\n",
    "First_Trans = np.zeros((num_of_unique_words + 1, num_of_unique_words + 1))\n",
    "Second_Trans = np.zeros((num_of_unique_words + 1, num_of_unique_words + 1))\n",
    "\n",
    "for playerLine in playerLines:\n",
    "    sentence = playerLine.split()\n",
    "    for i in range(len(sentence)):\n",
    "        if i < len(sentence) - 1:\n",
    "            First_Trans[words[sentence[i]]][words[sentence[i + 1]]] += 1\n",
    "        else:\n",
    "            First_Trans[words[sentence[i]]][num_of_unique_words] += 1\n",
    "\n",
    "        if i < len(sentence) - 2:\n",
    "            Second_Trans[words[sentence[i]]][words[sentence[i + 2]]] += 1\n",
    "        else:\n",
    "            Second_Trans[words[sentence[i]]][num_of_unique_words] += 1\n",
    "\n",
    "First_Trans[num_of_unique_words][num_of_unique_words] = 1\n",
    "Second_Trans[num_of_unique_words][num_of_unique_words] = 1\n",
    "\n",
    "First_Trans_Mat = First_Trans\n",
    "Second_Trans_Mat = Second_Trans\n",
    "\n",
    "for i in range(len(First_Trans)):\n",
    "    First_Trans_Mat[i] = First_Trans_Mat[i] / First_Trans_Mat[i].sum()\n",
    "    Second_Trans_Mat[i] = Second_Trans_Mat[i] / Second_Trans_Mat[i].sum()\n",
    "\n",
    "unique_words.append(None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "supreme-battlefield",
   "metadata": {},
   "source": [
    "# The idea is to implement Makrov Model to compute the next word, based on the probablilities transition of the last two states\n",
    "# Second-order Makrov Model\n",
    "# Hidden Makrov Model is assumed to be Makrov process with hidden (a.k.a unobserved) state\n",
    "# Therefore, I initally implemented Makrov Model, and I implemented Hidden Makrov Model later\n",
    "# I extended the Makrov Model with hidden states to become Hidden Makrov Model\n",
    "# My hidden states for this project are 'long' word and 'short' word\n",
    "# The average word length in English word length is 4.7 letters\n",
    "# Therefore, the word that is 4 or bellow letters is considered as 'Short'. Otherwise, that word is considered as 'Long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "miniature-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "# Emission probabilities\n",
    "def emission(prob, state):\n",
    "    for i in range(len(prob)-1):\n",
    "        if state == 'Short':\n",
    "            if len(unique_words[i]) < 5:\n",
    "                prob[i] = prob[i] * 2\n",
    "            else:\n",
    "                prob[i] = prob[i] / 2\n",
    "        else:\n",
    "            if len(unique_words[i]) > 4:\n",
    "                prob[i] = prob[i] * 2\n",
    "            else:\n",
    "                prob[i] = prob[i] / 2\n",
    "    prob[i] = prob[i] / prob.sum()\n",
    "    \n",
    "    \n",
    "Hiddent_State_Obj = {'Short': 0, 'Long': 1}\n",
    "Hiddent_States = ['Short','Long']\n",
    "Hidden_Mat = [[.8, .2],[.7, .3]]\n",
    "\n",
    "# Returns which of the Hidden State\n",
    "def nextState(Hidden_State):\n",
    "    nextState = np.random.choice(Hiddent_States, size = 1, p = Hidden_Mat[Hiddent_State_Obj[Hidden_State]])\n",
    "    return nextState[0]\n",
    "\n",
    "# Hidden Makrov Model implementation\n",
    "def HMM(word_choice = None, length = 7, sentence = [], hidden_State = 'Short'):\n",
    "    if word_choice is None:\n",
    "        word_choice = np.random.choice(new_words, size=1)[0]\n",
    "    if sentence == []:\n",
    "        sentence.append(word_choice)   \n",
    "    next_word = np.random.choice(unique_words, size = 1,p = First_Trans_Mat[words[sentence[-1]]])[0]\n",
    "    if length > 1:\n",
    "        while(next_word is None):\n",
    "            next_word = np.random.choice(unique_words, size = 1,p = First_Trans_Mat[words[sentence[-1]]])[0]\n",
    "\n",
    "    while next_word is not None:\n",
    "        sentence.append(next_word)\n",
    "        next_Probs = First_Trans_Mat[words[sentence[-1]]] * (Second_Trans_Mat[words[sentence[-2]]]) + First_Trans_Mat[words[sentence[-1]]]/4\n",
    "        next_prob = emission(next_Probs, hidden_State)\n",
    "        next_Probs[-1] += 0.00001\n",
    "        next_Probs = next_Probs / next_Probs.sum()\n",
    "        \n",
    "        if len(sentence) < length - 1:\n",
    "            if next_Probs.sum() > next_Probs[-1]:\n",
    "                next_Probs[-1] = next_Probs[-1] / 10\n",
    "            next_Probs = next_Probs / next_Probs.sum()\n",
    "            \n",
    "        if len(sentence) > length + 1:\n",
    "            next_Probs[-1] = next_Probs[-1] * 2\n",
    "            next_Probs = next_Probs / next_Probs.sum()\n",
    "        next_word = np.random.choice(unique_words, size = 1,p = next_Probs)[0]\n",
    "        \n",
    "        hidden_State = nextState(hidden_State)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "random-staff",
   "metadata": {},
   "source": [
    "# Generate a sentence with given words \"That's the letter \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "public-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's the letter come from the skin of a pair of our general?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"That's the letter \"\n",
    "generatedSentence = HMM(word_choice = sentence.split()[-1], sentence = sentence.split())\n",
    "print(' '.join(str(x) for x in generatedSentence if x is not None))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "atmospheric-shakespeare",
   "metadata": {},
   "source": [
    "# This generated sentence has a similar vibe of a player line written by Shakespeare. Thus, I'm pretty pleased with the current. However, I think the result could be better if I could try in a larger sample. 1000 lines is a relative small size sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
